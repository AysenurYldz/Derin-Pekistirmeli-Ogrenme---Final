# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I4xjLUJ589T3_Vp5Cc8imHq_87lnKBQy
"""

# -*- coding: utf-8 -*-
"""Optimize Edilmiş Tarım Drone'u Simülasyonu"""

import os
import cv2
import random
import numpy as np
import matplotlib.pyplot as plt
import time
import gym
from gym import spaces
from collections import deque
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.optim.lr_scheduler import StepLR
from google.colab import drive

# Google Drive bağlantısı
drive.mount("/content/drive")

# =====================
# SABİTLER ve AYARLAR
# =====================
GRID_SIZE = 6  # 6x6'lık tarım alanı
CELL_TYPES = {
    0: 'boş',      # Boş hücre
    1: 'sağlıklı', # Sağlıklı bitki
    2: 'hastalıklı' # Hastalıklı bitki
}

# =====================
# GÖRSELLERİN YÜKLENMESİ
# =====================
def resimleri_yukle():
    """Görsel dosyalarını yükler ve RGB formatına çevirir"""
    resim_klasoru = "/content/drive/MyDrive/24435004047_derin_pekiştirmeli_öğrenme/kodlar/images"
    resimler = {
        'tarla': cv2.imread(os.path.join(resim_klasoru, "tarla.png")),
        'bos': cv2.imread(os.path.join(resim_klasoru, "bos.png")),
        'hastalikli': cv2.imread(os.path.join(resim_klasoru, "kirmizi.png")),
        'drone': cv2.imread(os.path.join(resim_klasoru, "drone.png")),
        'sarj': cv2.imread(os.path.join(resim_klasoru, "sarj.png"))
    }

    # Görselleri RGB formatına çevir
    for key in resimler:
        if resimler[key] is not None:
            resimler[key] = cv2.cvtColor(resimler[key], cv2.COLOR_BGR2RGB)
        else:
            raise FileNotFoundError(f"{key} görseli bulunamadı: {os.path.join(resim_klasoru, key)}.png")

    return resimler

# Görselleri yükle
try:
    resimler = resimleri_yukle()
    HUCRE_Y, HUCRE_X = resimler['tarla'].shape[0]//GRID_SIZE, resimler['tarla'].shape[1]//GRID_SIZE
except Exception as hata:
    print(f"Görsel yükleme hatası: {str(hata)}")
    exit()

# =====================
# TARIM ALANI OLUŞTURMA
# =====================
def bitki_haritasi_olustur():
    """Rastgele bitki dağılımı oluşturur"""
    bitki_haritasi = np.ones((GRID_SIZE, GRID_SIZE), dtype=np.uint8)

    # 5 boş hücre
    bos_sayisi = 0
    while bos_sayisi < 5:
        i, j = random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)
        if bitki_haritasi[i, j] == 1 and (i, j) != (0, 0):  # Şarj istasyonunu (0,0) boş bırak
            bitki_haritasi[i, j] = 0
            bos_sayisi += 1

    # 5 hastalıklı bitki
    hastalikli_sayisi = 0
    while hastalikli_sayisi < 5:
        i, j = random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)
        if bitki_haritasi[i, j] == 1 and (i, j) != (0, 0):
            bitki_haritasi[i, j] = 2
            hastalikli_sayisi += 1

    return bitki_haritasi

# =====================
# ORTAM SINIFI (Optimize Edilmiş)
# =====================
class TarimOrtami(gym.Env):
    """Hastalıklı bitki tespitine odaklı tarım ortamı"""

    def __init__(self, goruntule=False):
        super(TarimOrtami, self).__init__()

        # Aksiyon uzayı: 0=yukarı, 1=aşağı, 2=sol, 3=sağ, 4=şarj
        self.action_space = spaces.Discrete(5)

        # Gözlem uzayı
        self.observation_space = spaces.Dict({
            "pozisyon": spaces.MultiDiscrete([GRID_SIZE, GRID_SIZE]),
            "ziyaret_edilen": spaces.MultiBinary([GRID_SIZE, GRID_SIZE]),
            "yerel_gorunum": spaces.Box(low=0, high=2, shape=(3,3), dtype=np.uint8),
            "batarya": spaces.Box(low=0, high=100, shape=(1,), dtype=np.float32),
            "sarjdan_beri_adim": spaces.Discrete(100),
            "hastalikli_tespit": spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)
        })

        self.goruntule = goruntule
        self.bitki_haritasi = bitki_haritasi_olustur()
        self.toplam_hastalikli = np.sum(self.bitki_haritasi == 2)

        # Batarya ayarları
        self.batarya_tuketim = 2.0  # Hareket başına batarya tüketimi (azaltıldı)
        self.sarj_hizi = 10.0         # Şarj hızı (artırıldı)
        self.maks_adim = 200         # Maksimum adım sayısı

        # Ödül parametreleri (Optimize Edildi)
        self.hastalikli_bulma_odulu = 100.0  # Azaltıldı
        self.eve_donus_odulu = 50.0          # Azaltıldı
        self.batarya_cezasi = -50.0           # Azaltıldı
        self.sarij_odulu = 5.0               # Azaltıldı
        self.bos_hucre_odulu = 10.0           # Azaltıldı
        self.saglikli_odulu = 5.0             # Azaltıldı
        self.ziyaret_cezasi = -2.0            # Yeni eklendi

        self.reset()

    def reset(self):
        """Ortamı sıfırla ve yeni harita oluştur"""
        self.bitki_haritasi = bitki_haritasi_olustur()
        self.toplam_hastalikli = np.sum(self.bitki_haritasi == 2)

        self.drone_pozisyonu = [0, 0]  # Başlangıç pozisyonu (şarj istasyonu)
        self.ziyaret_edilen = set()
        self.ziyaret_sayilari = np.zeros((GRID_SIZE, GRID_SIZE), dtype=np.int16)
        self.bulunan_hastalikli = 0
        self.batarya = 100.0
        self.adim_sayaci = 0
        self.sarjdan_beri_adim = 0
        self.gorev_tamamlandi = False

        return self._get_obs()

    def step(self, action):
        """Bir adım gerçekleştir"""
        self.adim_sayaci += 1
        y, x = self.drone_pozisyonu
        done = False
        reward = 0.0
        info = {'action': 'move', 'reason': None, 'mission_complete': False}

        # Batarya tüketimi
        if action != 4:
            self.batarya = max(0.0, self.batarya - self.batarya_tuketim)
            self.sarjdan_beri_adim += 1

        # Batarya kontrolü
        if self.batarya <= 0:
            reward = self.batarya_cezasi
            done = True
            info['reason'] = 'battery_depleted'
            return self._get_obs(), reward, done, info

        # Aksiyonları işle
        if action == 4:  # Şarj
            if (y, x) == (0, 0):
                charge_amount = min(self.sarj_hizi, 100 - self.batarya)
                self.batarya += charge_amount
                reward = max(self.sarij_odulu, charge_amount * 0.5)  # Azaltıldı
                info['action'] = 'charge'
                self.sarjdan_beri_adim = 0
            else:
                reward = -10.0  # Azaltıldı
                info['action'] = 'failed_charge'
        else:  # Hareket
            new_y, new_x = y, x
            if action == 0 and y > 0: new_y -= 1
            elif action == 1 and y < GRID_SIZE - 1: new_y += 1
            elif action == 2 and x > 0: new_x -= 1
            elif action == 3 and x < GRID_SIZE - 1: new_x += 1

            self.drone_pozisyonu = [new_y, new_x]
            self.ziyaret_sayilari[new_y, new_x] += 1

            # Ödüllendirme (Optimize Edildi)
            if (new_y, new_x) not in self.ziyaret_edilen:
                cell_type = self.bitki_haritasi[new_y, new_x]
                if cell_type == 2:  # Hastalıklı
                    reward = self.hastalikli_bulma_odulu
                    self.bulunan_hastalikli += 1
                elif cell_type == 0:  # Boş
                    reward = self.bos_hucre_odulu
                else:  # Sağlıklı
                    reward = self.saglikli_odulu
                self.ziyaret_edilen.add((new_y, new_x))
            else:
                reward = self.ziyaret_cezasi  # Sabit ceza

        # Görev tamamlama kontrolü
        if not self.gorev_tamamlandi and self.bulunan_hastalikli >= self.toplam_hastalikli:
            self.gorev_tamamlandi = True
            info['mission_complete'] = True
            reward += 50.0  # Azaltıldı

        # Eve dönüş teşviki
        if self.gorev_tamamlandi:
            if (y, x) == (0, 0):
                reward += self.eve_donus_odulu
                done = True
                info['reason'] = 'mission_complete'
            else:
                # Eve yaklaşma ödülü
                distance_to_home = y + x
                reward += max(2.0, 10.0 / (distance_to_home + 1))  # Azaltıldı

        # Maksimum adım kontrolü
        if self.adim_sayaci >= self.maks_adim:
            done = True
            info['reason'] = 'max_steps'

        return self._get_obs(), float(reward), done, info

    def _get_obs(self):
        """Gözlemleri hazırla"""
        y, x = self.drone_pozisyonu
        local_view = np.zeros((3,3), dtype=np.uint8)

        for i in [-1, 0, 1]:
            for j in [-1, 0, 1]:
                ny, nx = y+i, x+j
                if 0 <= ny < GRID_SIZE and 0 <= nx < GRID_SIZE:
                    local_view[i+1, j+1] = self.bitki_haritasi[ny, nx]

        visited_matrix = np.zeros((GRID_SIZE, GRID_SIZE), dtype=np.uint8)
        for zy, zx in self.ziyaret_edilen:
            visited_matrix[zy, zx] = 1

        tespit_orani = self.bulunan_hastalikli / max(1, self.toplam_hastalikli)

        return {
            "pozisyon": np.array([y, x]),
            "ziyaret_edilen": visited_matrix,
            "yerel_gorunum": local_view,
            "batarya": np.array([self.batarya]),
            "sarjdan_beri_adim": np.array([self.sarjdan_beri_adim]),
            "hastalikli_tespit": np.array([tespit_orani])
        }

    def render(self, mode='human'):
        """Ortamı görselleştir"""
        canvas = resimler['tarla'].copy()

        # Şarj istasyonu
        y1, y2 = 0, HUCRE_Y
        x1, x2 = 0, HUCRE_X
        canvas[y1:y2, x1:x2] = cv2.resize(resimler['sarj'], (HUCRE_X, HUCRE_Y))

        # Hücreleri çiz
        for i in range(GRID_SIZE):
            for j in range(GRID_SIZE):
                if (i, j) == (0, 0): continue

                y1, y2 = i*HUCRE_Y, (i+1)*HUCRE_Y
                x1, x2 = j*HUCRE_X, (j+1)*HUCRE_X

                cell_type = self.bitki_haritasi[i, j]
                if cell_type == 0:
                    canvas[y1:y2, x1:x2] = cv2.resize(resimler['bos'], (HUCRE_X, HUCRE_Y))
                elif cell_type == 2:
                    canvas[y1:y2, x1:x2] = cv2.resize(resimler['hastalikli'], (HUCRE_X, HUCRE_Y))

        # Drone'u çiz
        dy, dx = self.drone_pozisyonu
        y1, y2 = dy*HUCRE_Y, (dy+1)*HUCRE_Y
        x1, x2 = dx*HUCRE_X, (dx+1)*HUCRE_X
        canvas[y1:y2, x1:x2] = cv2.resize(resimler['drone'], (HUCRE_X, HUCRE_Y))

        plt.figure(figsize=(10, 10))
        plt.imshow(canvas)
        plt.axis('off')
        title = f"Adım: {self.adim_sayaci} | Batarya: {self.batarya:.1f}%\n"
        title += f"Hastalıklı: {self.bulunan_hastalikli}/{self.toplam_hastalikli} | "
        title += f"Son Şarj: {self.sarjdan_beri_adim} adım önce"
        plt.title(title)
        plt.show()

# =====================
# DERİN ÖĞRENME MODELİ (Optimize Edilmiş)
# =====================
class AdvancedDroneDQN(nn.Module):
    """Gelişmiş Derin Q-Ağı Modeli"""
    def __init__(self, input_dim, output_dim):
        super(AdvancedDroneDQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, output_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return self.fc4(x)

# =====================
# DENEY TAMPONU (Basitleştirilmiş)
# =====================
class ReplayBuffer:
    """Basit Deney Tamponu"""
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        if len(self.buffer) < batch_size:
            return None

        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)

# =====================
# OPTİMİZE EDİLMİŞ DRONE SINIFI
# =====================
class OptimizedDroneDQN:
    """Optimize Edilmiş Derin Öğrenmeli Drone Ajanı"""
    def __init__(self, ortam, input_dim, output_dim):
        self.ortam = ortam
        self.policy_net = AdvancedDroneDQN(input_dim, output_dim)
        self.target_net = AdvancedDroneDQN(input_dim, output_dim)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)
        self.scheduler = StepLR(self.optimizer, step_size=500, gamma=0.9)
        self.criterion = nn.MSELoss()

        # Deney tamponu
        self.buffer = ReplayBuffer(capacity=10000)
        self.batch_size = 64

        # Hiperparametreler (Optimize Edildi)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.target_update = 200
        self.step_count = 0

        # İstatistikler
        self.bolum_odulleri = []
        self.tespit_oranlari = []
        self.kayiplar = []

    def state_to_tensor(self, state):
        """Durum sözlüğünü tensöre çevir"""
        # Durum bileşenlerini birleştir
        state_vector = np.concatenate([
            state['pozisyon'] / GRID_SIZE,
            state['ziyaret_edilen'].flatten(),
            state['yerel_gorunum'].flatten() / 2.0,
            state['batarya'] / 100.0,
            [state['sarjdan_beri_adim'][0] / 100.0],
            state['hastalikli_tespit']
        ])
        return torch.tensor(state_vector, dtype=torch.float32).unsqueeze(0)

    def aksiyon_sec(self, state):
        """Epsilon-açgözlü politika ile aksiyon seç"""
        if random.random() < self.epsilon:
            return self.ortam.action_space.sample()

        state_tensor = self.state_to_tensor(state)
        with torch.no_grad():
            q_values = self.policy_net(state_tensor)
        return q_values.argmax(dim=1).item()

    def optimize_model(self):
        """Modeli optimize et"""
        if len(self.buffer) < self.batch_size:
            return None

        # Tampondan örnekle
        sample = self.buffer.sample(self.batch_size)
        if sample is None:
            return None

        states, actions, rewards, next_states, dones = sample

        # Verileri tensöre çevir
        state_batch = torch.cat([self.state_to_tensor(s) for s in states])
        action_batch = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)
        reward_batch = torch.tensor(rewards, dtype=torch.float32)
        next_state_batch = torch.cat([self.state_to_tensor(s) for s in next_states])
        done_batch = torch.tensor(dones, dtype=torch.float32)

        # Q değerlerini hesapla
        q_values = self.policy_net(state_batch).gather(1, action_batch)

        # Hedef Q değerlerini hesapla
        with torch.no_grad():
            next_q_values = self.target_net(next_state_batch).max(1)[0]
            expected_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)

        # Kaybı hesapla
        loss = self.criterion(q_values.squeeze(), expected_q_values)

        # Modeli güncelle
        self.optimizer.zero_grad()
        loss.backward()

        # Gradyan kırpma
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)

        self.optimizer.step()

        # Hedef ağı güncelle
        self.step_count += 1
        if self.step_count % self.target_update == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())

        # Epsilon'u azalt
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

        return loss.item()

    def egit(self, bolum_sayisi=1000, maks_adim=200):
        """Ajanı eğit"""
        self.ortam.maks_adim = maks_adim
        baslangic_zamani = time.time()
        en_iyi_odul = -float('inf')

        for bolum in range(1, bolum_sayisi + 1):
            durum = self.ortam.reset()
            toplam_odul = 0.0
            done = False
            kayip = 0.0
            optim_sayisi = 0

            while not done:
                aksiyon = self.aksiyon_sec(durum)
                sonraki_durum, odul, done, bilgi = self.ortam.step(aksiyon)

                # Deneyi tampona kaydet
                self.buffer.push(durum, aksiyon, odul, sonraki_durum, done)

                # Modeli optimize et
                loss = self.optimize_model()
                if loss:
                    kayip = loss
                    optim_sayisi += 1

                toplam_odul += odul
                durum = sonraki_durum

            # Öğrenme oranını güncelle
            self.scheduler.step()

            # İstatistikleri güncelle
            tespit_orani = self.ortam.bulunan_hastalikli / self.ortam.toplam_hastalikli

            self.bolum_odulleri.append(toplam_odul)
            self.tespit_oranlari.append(tespit_orani)

            if optim_sayisi > 0:
                ortalama_kayip = kayip / optim_sayisi
                self.kayiplar.append(ortalama_kayip)
            else:
                ortalama_kayip = 0.0

            # İlerleme raporu
            if bolum % 100 == 0:
                ortalama_odul = np.mean(self.bolum_odulleri[-100:])
                ortalama_tespit = np.mean(self.tespit_oranlari[-100:]) * 100
                gecen_zaman = (time.time() - baslangic_zamani) / 60

                # En iyi modeli kaydet
                if ortalama_odul > en_iyi_odul:
                    en_iyi_odul = ortalama_odul
                    self.model_kaydet(f"/content/drive/MyDrive/24435004047_derin_pekiştirmeli_öğrenme/kodlar/models/en_iyi_drone_dqn.pth")

                print(f"Bölüm {bolum} | Ort. Ödül: {ortalama_odul:.1f} | Kayıp: {ortalama_kayip:.4f}")
                print(f"Tespit: {ortalama_tespit:.1f}% | Epsilon: {self.epsilon:.4f}")
                print(f"LR: {self.optimizer.param_groups[0]['lr']:.6f} | Süre: {gecen_zaman:.1f} dakika")
                print("----------------------------------")

            # Ara model kaydet
            if bolum % 500 == 0:
                self.model_kaydet(f"/content/drive/MyDrive/24435004047_derin_pekiştirmeli_öğrenme/kodlar/models/drone_dqn_{bolum}.pth")

        self.model_kaydet()
        self.egitim_grafikleri()

    def egitim_grafikleri(self):
        """Eğitim istatistiklerini görselleştir"""
        plt.figure(figsize=(15, 5))

        # Ödül grafiği
        plt.subplot(1, 3, 1)
        plt.plot(np.convolve(self.bolum_odulleri, np.ones(100)/100, mode='valid'))
        plt.title('100 Bölüm Ortalama Ödül')
        plt.xlabel('Bölüm')
        plt.ylabel('Ödül')

        # Tespit oranı grafiği
        plt.subplot(1, 3, 2)
        plt.plot(np.array(self.tespit_oranlari) * 100)
        plt.title('Tespit Oranı')
        plt.xlabel('Bölüm')
        plt.ylabel('Tespit Oranı (%)')
        plt.ylim(0, 100)

        # Kayıp grafiği
        plt.subplot(1, 3, 3)
        plt.plot(self.kayiplar)
        plt.title('Eğitim Kaybı')
        plt.xlabel('Bölüm')
        plt.ylabel('Kayıp')

        plt.tight_layout()
        plt.savefig("/content/drive/MyDrive/24435004047_derin_pekiştirmeli_öğrenme/kodlar/models/egitim_grafikleri.png")
        plt.show()

    def model_kaydet(self, dosya_adi="/content/drive/MyDrive/24435004047_derin_pekiştirmeli_öğrenme/kodlar/models/drone_dqn.pth"):
        """Modeli kaydet"""
        torch.save({
            'policy_state_dict': self.policy_net.state_dict(),
            'target_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epsilon': self.epsilon,
        }, dosya_adi)
        print(f"Model '{dosya_adi}' olarak kaydedildi.")

    def model_yukle(self, dosya_adi):
        """Modeli yükle"""
        checkpoint = torch.load(dosya_adi, map_location=torch.device('cpu'))
        self.policy_net.load_state_dict(checkpoint['policy_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.epsilon = checkpoint['epsilon']
        print(f"Model '{dosya_adi}' başarıyla yüklendi.")

# =====================
# TEST FONKSİYONU
# =====================
def drone_testi(model=None, test_sayisi=5, bekleme_suresi=0.3):
    """Eğitilmiş ajanı test et"""
    test_ortami = TarimOrtami(goruntule=True)

    # Durum boyutunu hesapla
    ornek_durum = test_ortami.reset()
    input_dim = len(np.concatenate([
        ornek_durum['pozisyon'] / GRID_SIZE,
        ornek_durum['ziyaret_edilen'].flatten(),
        ornek_durum['yerel_gorunum'].flatten() / 2.0,
        ornek_durum['batarya'] / 100.0,
        [ornek_durum['sarjdan_beri_adim'][0] / 100.0],
        ornek_durum['hastalikli_tespit']
    ]))
    output_dim = test_ortami.action_space.n

    ajan = OptimizedDroneDQN(test_ortami, input_dim, output_dim)

    if model is not None:
        ajan.model_yukle(model)
    ajan.epsilon = 0.01  # Keşif oranını düşür

    tespit_oranlari = []
    toplam_oduller = []

    for test in range(1, test_sayisi+1):
        durum = test_ortami.reset()
        done = False
        toplam_odul = 0.0

        print(f"\n=== TEST {test} ===")
        test_ortami.render()

        while not done:
            aksiyon = ajan.aksiyon_sec(durum)
            durum, odul, done, bilgi = test_ortami.step(aksiyon)
            toplam_odul += odul

            if test_ortami.goruntule:
                test_ortami.render()
                time.sleep(bekleme_suresi)

            print(f"Adım {test_ortami.adim_sayaci}: {bilgi['action']}, Ödül: {odul:.1f}, Batarya: {durum['batarya'][0]:.1f}%")

        # İstatistikleri kaydet
        tespit_orani = test_ortami.bulunan_hastalikli / test_ortami.toplam_hastalikli

        tespit_oranlari.append(tespit_orani)
        toplam_oduller.append(toplam_odul)

        print(f"\nTEST SONUCU | Toplam Ödül: {toplam_odul:.1f}")
        print(f"Hastalıklı Tespit: {test_ortami.bulunan_hastalikli}/{test_ortami.toplam_hastalikli}")
        print("="*50)

    # Genel sonuçlar
    print("\n=== GENEL TEST SONUÇLARI ===")
    print(f"Ortalama Tespit Oranı: {np.mean(tespit_oranlari)*100:.1f}%")
    print(f"Ortalama Toplam Ödül: {np.mean(toplam_oduller):.1f}")

# =====================
# ANA PROGRAM
# =====================
if __name__ == "__main__":
    # Ortamı oluştur
    ortam = TarimOrtami(goruntule=False)

    # Durum boyutunu hesapla
    ornek_durum = ortam.reset()
    input_dim = len(np.concatenate([
        ornek_durum['pozisyon'] / GRID_SIZE,
        ornek_durum['ziyaret_edilen'].flatten(),
        ornek_durum['yerel_gorunum'].flatten() / 2.0,
        ornek_durum['batarya'] / 100.0,
        [ornek_durum['sarjdan_beri_adim'][0] / 100.0],
        ornek_durum['hastalikli_tespit']
    ]))
    output_dim = ortam.action_space.n

    # Her zaman yeni eğitime başla
    print("Yeni eğitim başlatılıyor...")

    # Derin öğrenme ajanını oluştur ve eğit
    ajan = OptimizedDroneDQN(ortam, input_dim, output_dim)
    ajan.egit(bolum_sayisi=1500)

    # Eğitilmiş modeli test et
    drone_testi("/content/drive/MyDrive/24435004047_derin_pekiştirmeli_öğrenme/kodlar/models/en_iyi_drone_dqn.pth", test_sayisi=3)

